{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: using random word inputs, predict which South Park character is speaking from a list of top characters\n",
    "\n",
    "### Data source: https://www.kaggle.com/tovarischsukhov/southparklines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>70896</td>\n",
       "      <td>70896</td>\n",
       "      <td>70896</td>\n",
       "      <td>70896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3950</td>\n",
       "      <td>64301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>What?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6416</td>\n",
       "      <td>5271</td>\n",
       "      <td>9774</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Season Episode Character     Line\n",
       "count   70896   70896     70896    70896\n",
       "unique     19      19      3950    64301\n",
       "top         2      10   Cartman  What?\\n\n",
       "freq     6416    5271      9774      361"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "South_Park_raw = pd.read_csv('All-seasons.csv')\n",
    "South_Park_raw.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Season Episode Character                                               Line\n",
      "0     10       1      Stan         You guys, you guys! Chef is going away. \\n\n",
      "1     10       1      Kyle                        Going away? For how long?\\n\n",
      "2     10       1      Stan                                         Forever.\\n\n",
      "3     10       1      Chef                                  I'm sorry boys.\\n\n",
      "4     10       1      Stan  Chef said he's been bored, so he joining a gro...\n",
      "(70896, 4)\n"
     ]
    }
   ],
   "source": [
    "# Head and shape of dataset\n",
    "print(South_Park_raw.head())\n",
    "print(South_Park_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Season Episode Character     Line\n",
      "count   70896   70896     70896    70896\n",
      "unique     19      19      3950    64301\n",
      "top         2      10   Cartman  What?\\n\n",
      "freq     6416    5271      9774      361\n"
     ]
    }
   ],
   "source": [
    "print (South_Park_raw.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character\n",
      "Cartman         9774\n",
      "Stan            7680\n",
      "Kyle            7099\n",
      "Butters         2602\n",
      "Randy           2467\n",
      "Mr. Garrison    1002\n",
      "Chef             917\n",
      "Kenny            881\n",
      "Sharon           862\n",
      "Mr. Mackey       633\n",
      "Gerald           626\n",
      "Jimmy            597\n",
      "Wendy            585\n",
      "Liane            582\n",
      "Sheila           566\n",
      "Jimbo            556\n",
      "dtype: int64\n",
      "       Character     Line\n",
      "count      37429    37429\n",
      "unique        16    34196\n",
      "top      Cartman  What?\\n\n",
      "freq        9774      237\n"
     ]
    }
   ],
   "source": [
    "#Select just speakers with more than 500 lines\n",
    "\n",
    "top_speakers = South_Park_raw.groupby(['Character']).size().loc[South_Park_raw.groupby(['Character']).size() > 500]\n",
    "print (top_speakers.sort_values(ascending=False))\n",
    "\n",
    "#Select rows top speakers   \n",
    "\"\"\" This is the dataset we will be working with\"\"\"\n",
    "\n",
    "main_char_lines = pd.DataFrame(South_Park_raw.loc[South_Park_raw['Character'].isin(top_speakers.index.values)])\n",
    "del main_char_lines['Season']\n",
    "del main_char_lines['Episode']\n",
    "\n",
    "main_char_lines = main_char_lines.reset_index(drop=True)\n",
    "\n",
    "print (main_char_lines.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X = main_char_lines.Line\n",
    "y = main_char_lines.Character\n",
    "\n",
    "#print (y.value_counts(normalize=True))\n",
    "\n",
    "# split the new DataFrame into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for best parameters to use in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "#pipe.steps\n",
    "\n",
    "#param_grid = {}\n",
    "#param_grid[\"tfidfvectorizer__max_features\"] = [500, 1000, 15000]\n",
    "#param_grid[\"tfidfvectorizer__ngram_range\"] = [(1,1), (1,2), (2,2)]\n",
    "#param_grid[\"tfidfvectorizer__lowercase\"] = [True, False]\n",
    "#param_grid[\"tfidfvectorizer__stop_words\"] = [\"english\", None]\n",
    "#param_grid[\"tfidfvectorizer__strip_accents\"] = [\"ascii\", \"unicode\", None]\n",
    "#param_grid[\"tfidfvectorizer__analyzer\"] = [\"word\", \"char\"]\n",
    "#param_grid[\"tfidfvectorizer__binary\"] = [True, False]\n",
    "#param_grid[\"tfidfvectorizer__norm\"] = [\"l1\", \"l2\", None]\n",
    "#param_grid[\"tfidfvectorizer__use_idf\"] = [True, False]\n",
    "#param_grid[\"tfidfvectorizer__smooth_idf\"] = [True, False]\n",
    "#param_grid[\"tfidfvectorizer__sublinear_tf\"] = [True, False]\n",
    "\n",
    "#grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "#Helpful for understanding how to create your param grid.\n",
    "#grid.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (This can take a while to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(grid.best_params_)\n",
    "#print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy within dataset:  0.40530070266370993\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(analyzer='word', stop_words='english', max_features = 850, ngram_range=(1, 1), \n",
    "                       binary=False, lowercase=True, norm=None, smooth_idf=True, strip_accents=None,\n",
    "                       sublinear_tf=True, use_idf=False)\n",
    "\n",
    "mcl_transformed = vect.fit_transform(X)\n",
    "\n",
    "nb_SP_Model = MultinomialNB()\n",
    "nb_SP_Model.fit(mcl_transformed, y)\n",
    "print (\"Model accuracy within dataset: \", nb_SP_Model.score(mcl_transformed, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy with cross validation: 0.3333783091995065\n"
     ]
    }
   ],
   "source": [
    "print (\"Model accuracy with cross validation:\", cross_val_score(MultinomialNB(), mcl_transformed.toarray(), \n",
    "                                                                y, cv=5, scoring=\"accuracy\").mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kyle']  most likely said it.\n"
     ]
    }
   ],
   "source": [
    "# Predict on new text\n",
    "new_text = [\"I like Stan\"]\n",
    "new_text_transform = vect.transform(new_text)\n",
    "\n",
    "print (nb_SP_Model.predict(new_text_transform),\" most likely said it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table with Characters' Line likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Character  Likelihood\n",
      "7           Kyle    0.310769\n",
      "1        Cartman    0.216608\n",
      "11         Randy    0.204456\n",
      "0        Butters    0.069615\n",
      "15         Wendy    0.061166\n",
      "12        Sharon    0.037349\n",
      "14          Stan    0.031404\n",
      "2           Chef    0.014917\n",
      "5          Jimmy    0.013817\n",
      "10    Mr. Mackey    0.008807\n",
      "6          Kenny    0.008302\n",
      "9   Mr. Garrison    0.008297\n",
      "4          Jimbo    0.006116\n",
      "3         Gerald    0.005636\n",
      "8          Liane    0.001397\n",
      "13        Sheila    0.001343\n"
     ]
    }
   ],
   "source": [
    "SP_prob=pd.DataFrame(nb_SP_Model.predict_proba(new_text_transform))\n",
    "SP_prob=pd.DataFrame.transpose(SP_prob)\n",
    "SP_prob.columns = ['Likelihood']\n",
    "\n",
    "top_speakers_index = top_speakers.reset_index()\n",
    "top_speakers_index.columns = ['Character', 'Lines']\n",
    "top_speakers_index = top_speakers_index.drop('Lines', 1)\n",
    "\n",
    "Result = pd.concat([top_speakers_index, SP_prob], axis=1)\n",
    "\n",
    "print (Result.sort_values('Likelihood',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<37429x850 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 130687 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcl_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate \"spamminess\" for the top 3 characters: Cartman, Stan and Kyle\n",
    "### Used to test common words pertaining to these characters more than to others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate \"spaminess\" for Cartman with detailed coding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Character                                               Line\n",
      "0      Not Cartman         You guys, you guys! Chef is going away. \\n\n",
      "1      Not Cartman                        Going away? For how long?\\n\n",
      "2      Not Cartman                                         Forever.\\n\n",
      "3      Not Cartman                                  I'm sorry boys.\\n\n",
      "4      Not Cartman  Chef said he's been bored, so he joining a gro...\n",
      "5      Not Cartman                                             Wow!\\n\n",
      "7      Not Cartman     What's the meaning of life? Why are we here?\\n\n",
      "9          Cartman  I'm gonna miss him.  I'm gonna miss Chef and I...\n",
      "10     Not Cartman  Dude, how are we gonna go on? Chef was our fuh...\n",
      "12     Not Cartman                                         Bye-bye!\\n\n",
      "13     Not Cartman                                        Good-bye!\\n\n",
      "14     Not Cartman                                         So long!\\n\n",
      "17     Not Cartman  Good-bye, Chef! Have a great time with the Sup...\n",
      "18     Not Cartman                                     Good-bye! ..\\n\n",
      "19     Not Cartman                           Draw two card, fatass.\\n\n",
      "20         Cartman                            Reverse to you, Jew. \\n\n",
      "21     Not Cartman                                    I'll get it. \\n\n",
      "22     Not Cartman                           Hello there, children!\\n\n",
      "23     Not Cartman                                       He's back!\\n\n",
      "24     Not Cartman                                            Yeah!\\n\n",
      "25         Cartman                                      All right! \\n\n",
      "26     Not Cartman               Chef! I can't believe you're back!\\n\n",
      "27     Not Cartman                                 Well, it's true.\\n\n",
      "28     Not Cartman                       But are you back for good?\\n\n",
      "29     Not Cartman                                    That's right.\\n\n",
      "32     Not Cartman                                    Oh, finally! \\n\n",
      "33     Not Cartman  Wow! It seems like you had a great time with t...\n",
      "34     Not Cartman                                            Yeah!\\n\n",
      "36     Not Cartman                                           Nnono!\\n\n",
      "37     Not Cartman  Ohhh, so have you decided you can still belong...\n",
      "...            ...                                                ...\n",
      "70855  Not Cartman  But I put my faith in a higher power and... I ...\n",
      "70857  Not Cartman         Oh wait. Sh sh. Hold on a second, gang. \\n\n",
      "70861  Not Cartman                                            What?\\n\n",
      "70864  Not Cartman  That means... I'm not cured. I still have the ...\n",
      "70866  Not Cartman  You heard what he said!  The higher power didn...\n",
      "70871  Not Cartman                                  Dad, Dad, Stop!\\n\n",
      "70872  Not Cartman               I'm sorry, son! I'm off the wagon!\\n\n",
      "70873  Not Cartman  Dad, you don't have to do this! You have the p...\n",
      "70874  Not Cartman                 But the statue wasn't a miracle!\\n\n",
      "70875  Not Cartman  Yeah. The statue wasn't a miracle, Dad. So tha...\n",
      "70876  Not Cartman  You're right, Stan. If God didn't make me stop...\n",
      "70877  Not Cartman                                              No!\\n\n",
      "70878  Not Cartman                                             No??\\n\n",
      "70879  Not Cartman  Dad, you like to drink. So have a drink once i...\n",
      "70880  Not Cartman  But, maybe... I'm just the kind of person who ...\n",
      "70881  Not Cartman  Naw. All or nothing is easy. But learning to d...\n",
      "70882  Not Cartman      How did I manage to raise such a smart kid?\\n\n",
      "70883  Not Cartman                        I've had a great teacher.\\n\n",
      "70884  Not Cartman                                      Thanks son.\\n\n",
      "70885  Not Cartman  No not you, my karate teacher. He's really sma...\n",
      "70886  Not Cartman  Oh. Well, tell you what: let's leave the car h...\n",
      "70887  Not Cartman                                       All right!\\n\n",
      "70888  Not Cartman       Come on!  Or maybe I'll have three beers. \\n\n",
      "70889  Not Cartman       That's probably okay if you spread it out.\\n\n",
      "70890  Not Cartman                             Well how about four?\\n\n",
      "70891  Not Cartman                       I think you're pushing it.\\n\n",
      "70892  Not Cartman                                How about twenty?\\n\n",
      "70893  Not Cartman                           That's not disciprine.\\n\n",
      "70894  Not Cartman                   Right right. Does vodka count?\\n\n",
      "70895  Not Cartman                                             Dad!\\n\n",
      "\n",
      "[37429 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "cartman = pd.DataFrame(South_Park_raw.loc[South_Park_raw['Character'].isin(top_speakers.index.values)])\n",
    "del cartman['Season']\n",
    "del cartman['Episode']\n",
    "\n",
    "cartman.Character[cartman.Character != 'Cartman'] = 'Not Cartman'\n",
    "cartman.Character[cartman.Character == 'Cartman'] = 'Cartman'\n",
    "print (cartman)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Not Cartman    0.738866\n",
       "Cartman        0.261134\n",
       "Name: Character, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cartman.Character.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8318950546367789"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cartman = cartman.Line\n",
    "y_cartman = cartman.Character\n",
    "vect_cartman =CountVectorizer(stop_words='english')\n",
    "Xdtm_cartman = vect_cartman.fit_transform(X_cartman)\n",
    "nb_cartman = MultinomialNB()\n",
    "nb_cartman.fit(Xdtm_cartman,y_cartman)\n",
    "nb_cartman.score(Xdtm_cartman,y_cartman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15271"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_cartman = vect_cartman.get_feature_names()\n",
    "len(tokens_cartman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '10', '100', '1000', '102', '104', '105', '106', '10th', '11', '12', '12mm', '12th', '13', '1340s', '13th', '14', '1421', '15', '16', '160', '1621', '167', '17', '1776', '18', '182', '19', '1924', '1956', '1960s', '1972', '1973', '1984', '1999', '20', '200', '2000', '2001', '2004', '2008you', '2009', '2010', '2012', '203', '21', '212', '213', '214', '22']\n"
     ]
    }
   ],
   "source": [
    "print (vect_cartman.get_feature_names()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3., 19.,  0., ...,  0.,  0.,  0.],\n",
       "       [13., 21.,  5., ...,  1.,  1.,  2.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_cartman.feature_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 15271)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_cartman.feature_count_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3., 19.,  0., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_count_cartman= nb_cartman.feature_count_[0,:]\n",
    "token_count_cartman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13., 21.,  5., ...,  1.,  1.,  2.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_count_not_cartman = nb_cartman.feature_count_[1, :]\n",
    "token_count_not_cartman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cartman</th>\n",
       "      <th>Not_Cartman</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>programming</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>braved</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exploit</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poor</th>\n",
       "      <td>92.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awwwwrrr</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>davin</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slots</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rl</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>babysitters</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reporter</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Cartman  Not_Cartman\n",
       "token                            \n",
       "programming      0.0          1.0\n",
       "braved           0.0          1.0\n",
       "exploit          2.0          2.0\n",
       "poor            92.0         65.0\n",
       "awwwwrrr         1.0          0.0\n",
       "davin            4.0          0.0\n",
       "slots            0.0          2.0\n",
       "rl               1.0          0.0\n",
       "babysitters      1.0          1.0\n",
       "reporter         0.0          3.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame of tokens with their separate Not-Cartman and Cartman counts\n",
    "cartman_tokens = pd.DataFrame({'token':tokens_cartman, 'Cartman':token_count_cartman, 'Not_Cartman':token_count_not_cartman}).set_index('token')\n",
    "cartman_tokens.sample(10, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cartman</th>\n",
       "      <th>Not_Cartman</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>programming</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>braved</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exploit</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poor</th>\n",
       "      <td>93.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awwwwrrr</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>davin</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slots</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rl</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>babysitters</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reporter</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Cartman  Not_Cartman\n",
       "token                            \n",
       "programming      1.0          2.0\n",
       "braved           1.0          2.0\n",
       "exploit          3.0          3.0\n",
       "poor            93.0         66.0\n",
       "awwwwrrr         2.0          1.0\n",
       "davin            5.0          1.0\n",
       "slots            1.0          3.0\n",
       "rl               2.0          1.0\n",
       "babysitters      2.0          2.0\n",
       "reporter         1.0          4.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add 1 to Cartman and Not Cartman counts to avoid dividing by 0\n",
    "cartman_tokens['Cartman'] = cartman_tokens.Cartman + 1\n",
    "cartman_tokens['Not_Cartman'] = cartman_tokens.Not_Cartman + 1\n",
    "cartman_tokens.sample(10, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9774., 27655.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes counts the number of observations in each class\n",
    "nb_cartman.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cartman</th>\n",
       "      <th>Not_Cartman</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>programming</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>braved</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exploit</th>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poor</th>\n",
       "      <td>0.009515</td>\n",
       "      <td>0.002387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awwwwrrr</th>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>davin</th>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slots</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rl</th>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>babysitters</th>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reporter</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Cartman  Not_Cartman\n",
       "token                             \n",
       "programming  0.000102     0.000072\n",
       "braved       0.000102     0.000072\n",
       "exploit      0.000307     0.000108\n",
       "poor         0.009515     0.002387\n",
       "awwwwrrr     0.000205     0.000036\n",
       "davin        0.000512     0.000036\n",
       "slots        0.000102     0.000108\n",
       "rl           0.000205     0.000036\n",
       "babysitters  0.000205     0.000072\n",
       "reporter     0.000102     0.000145"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the Cartman and Not Cartman counts into frequencies\n",
    "cartman_tokens['Cartman'] = cartman_tokens.Cartman / nb_cartman.class_count_[0]\n",
    "cartman_tokens['Not_Cartman'] = cartman_tokens.Not_Cartman / nb_cartman.class_count_[1]\n",
    "cartman_tokens.sample(10, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cartman</th>\n",
       "      <th>Not_Cartman</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>programming</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>1.414723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>braved</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>1.414723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exploit</th>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>2.829445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poor</th>\n",
       "      <td>0.009515</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>3.986946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awwwwrrr</th>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>5.658891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>davin</th>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>14.147227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slots</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.943148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rl</th>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>5.658891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>babysitters</th>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>2.829445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reporter</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.707361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Cartman  Not_Cartman  spam_ratio\n",
       "token                                         \n",
       "programming  0.000102     0.000072    1.414723\n",
       "braved       0.000102     0.000072    1.414723\n",
       "exploit      0.000307     0.000108    2.829445\n",
       "poor         0.009515     0.002387    3.986946\n",
       "awwwwrrr     0.000205     0.000036    5.658891\n",
       "davin        0.000512     0.000036   14.147227\n",
       "slots        0.000102     0.000108    0.943148\n",
       "rl           0.000205     0.000036    5.658891\n",
       "babysitters  0.000205     0.000072    2.829445\n",
       "reporter     0.000102     0.000145    0.707361"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the ratio of Cartman-to-Not_Cartman for each token\n",
    "cartman_tokens['spam_ratio'] = cartman_tokens.Cartman / cartman_tokens.Not_Cartman\n",
    "cartman_tokens.sample(10, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cartman</th>\n",
       "      <th>Not_Cartman</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nyah</th>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>67.906691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kewl</th>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>67.906691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wicky</th>\n",
       "      <td>0.002149</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>59.418355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sail</th>\n",
       "      <td>0.002046</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>56.588909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polly</th>\n",
       "      <td>0.001944</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>53.759464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunh</th>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>45.271127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oink</th>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>45.271127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sucky</th>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>39.612237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smurf</th>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>36.782791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cartmaaanbrah</th>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>36.782791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Cartman  Not_Cartman  spam_ratio\n",
       "token                                           \n",
       "nyah           0.002455     0.000036   67.906691\n",
       "kewl           0.002455     0.000036   67.906691\n",
       "wicky          0.002149     0.000036   59.418355\n",
       "sail           0.002046     0.000036   56.588909\n",
       "polly          0.001944     0.000036   53.759464\n",
       "tunh           0.001637     0.000036   45.271127\n",
       "oink           0.001637     0.000036   45.271127\n",
       "sucky          0.001432     0.000036   39.612237\n",
       "smurf          0.001330     0.000036   36.782791\n",
       "cartmaaanbrah  0.001330     0.000036   36.782791"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the DataFrame sorted by spam_ratio\n",
    "cartman_tokens.sort_values('spam_ratio', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.90669122160834"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try looking up scores of different words\n",
    "word = \"nyah\"\n",
    "cartman_tokens.loc[word, 'spam_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Spamminess\" for Stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stan</th>\n",
       "      <th>Not_Stan</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>randy</th>\n",
       "      <td>0.006824</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>52.406467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sharon</th>\n",
       "      <td>0.005345</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>41.047430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fellas</th>\n",
       "      <td>0.004639</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>35.626071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eric</th>\n",
       "      <td>0.016975</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>32.592692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>children</th>\n",
       "      <td>0.014521</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>27.881273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stanley</th>\n",
       "      <td>0.007093</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>27.235873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dreidel</th>\n",
       "      <td>0.003395</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>26.074154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jews</th>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>21.169115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kitty</th>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>18.329356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awesom</th>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>17.038556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hon</th>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>15.231436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boys</th>\n",
       "      <td>0.009849</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>15.128172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mommy</th>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>12.907997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mkay</th>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>11.617197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hippies</th>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>11.617197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nuh</th>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>11.359037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heck</th>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>11.100877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stan</th>\n",
       "      <td>0.032707</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>10.466234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heheh</th>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>10.068238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thirty</th>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>10.068238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ike</th>\n",
       "      <td>0.007832</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>10.025211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student</th>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>9.810078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>society</th>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>9.551918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oo</th>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>9.551918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jew</th>\n",
       "      <td>0.004941</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>9.487378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sweetie</th>\n",
       "      <td>0.002420</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>9.293758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kay</th>\n",
       "      <td>0.012068</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>9.267942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economy</th>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>9.035598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hippie</th>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>8.777438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bitches</th>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>8.777438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>takanawa</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.086053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grampa</th>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.075297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>omigod</th>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.073760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immigrants</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tralalalala</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syne</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rhodie</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>religions</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manuel</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acknowledge</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trophy</th>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fie</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bluuch</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pubic</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaaaaaaaaaaaaa</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buttpipe</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pff</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>packing</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>headgear</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.064540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>critters</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.051632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lions</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.051632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auld</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.043027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cheeseburger</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.043027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clang</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.043027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gorak</th>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>0.040762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yorker</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.036880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleech</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.036880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>melvins</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.036880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sparky</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>0.009929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15271 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Stan  Not_Stan  spam_ratio\n",
       "token                                           \n",
       "randy             0.006824  0.000130   52.406467\n",
       "sharon            0.005345  0.000130   41.047430\n",
       "fellas            0.004639  0.000130   35.626071\n",
       "eric              0.016975  0.000521   32.592692\n",
       "children          0.014521  0.000521   27.881273\n",
       "stanley           0.007093  0.000260   27.235873\n",
       "dreidel           0.003395  0.000130   26.074154\n",
       "jews              0.002756  0.000130   21.169115\n",
       "kitty             0.002387  0.000130   18.329356\n",
       "awesom            0.002219  0.000130   17.038556\n",
       "hon               0.001983  0.000130   15.231436\n",
       "boys              0.009849  0.000651   15.128172\n",
       "mommy             0.001681  0.000130   12.907997\n",
       "mkay              0.001513  0.000130   11.617197\n",
       "hippies           0.001513  0.000130   11.617197\n",
       "nuh               0.001479  0.000130   11.359037\n",
       "heck              0.001445  0.000130   11.100877\n",
       "stan              0.032707  0.003125   10.466234\n",
       "heheh             0.001311  0.000130   10.068238\n",
       "thirty            0.001311  0.000130   10.068238\n",
       "ike               0.007832  0.000781   10.025211\n",
       "student           0.001277  0.000130    9.810078\n",
       "society           0.001244  0.000130    9.551918\n",
       "oo                0.001244  0.000130    9.551918\n",
       "jew               0.004941  0.000521    9.487378\n",
       "sweetie           0.002420  0.000260    9.293758\n",
       "kay               0.012068  0.001302    9.267942\n",
       "economy           0.001177  0.000130    9.035598\n",
       "hippie            0.001143  0.000130    8.777438\n",
       "bitches           0.001143  0.000130    8.777438\n",
       "...                    ...       ...         ...\n",
       "takanawa          0.000034  0.000391    0.086053\n",
       "grampa            0.000235  0.003125    0.075297\n",
       "omigod            0.000202  0.002734    0.073760\n",
       "immigrants        0.000034  0.000521    0.064540\n",
       "tralalalala       0.000034  0.000521    0.064540\n",
       "syne              0.000034  0.000521    0.064540\n",
       "rhodie            0.000034  0.000521    0.064540\n",
       "religions         0.000034  0.000521    0.064540\n",
       "manuel            0.000034  0.000521    0.064540\n",
       "acknowledge       0.000034  0.000521    0.064540\n",
       "trophy            0.000067  0.001042    0.064540\n",
       "fie               0.000034  0.000521    0.064540\n",
       "lang              0.000034  0.000521    0.064540\n",
       "bluuch            0.000034  0.000521    0.064540\n",
       "pubic             0.000034  0.000521    0.064540\n",
       "aaaaaaaaaaaaaaaa  0.000034  0.000521    0.064540\n",
       "buttpipe          0.000034  0.000521    0.064540\n",
       "pff               0.000034  0.000521    0.064540\n",
       "packing           0.000034  0.000521    0.064540\n",
       "headgear          0.000034  0.000521    0.064540\n",
       "critters          0.000034  0.000651    0.051632\n",
       "lions             0.000034  0.000651    0.051632\n",
       "auld              0.000034  0.000781    0.043027\n",
       "cheeseburger      0.000034  0.000781    0.043027\n",
       "clang             0.000034  0.000781    0.043027\n",
       "gorak             0.000101  0.002474    0.040762\n",
       "yorker            0.000034  0.000911    0.036880\n",
       "bleech            0.000034  0.000911    0.036880\n",
       "melvins           0.000034  0.000911    0.036880\n",
       "sparky            0.000034  0.003385    0.009929\n",
       "\n",
       "[15271 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stan = pd.DataFrame(South_Park_raw.loc[South_Park_raw['Character'].isin(top_speakers.index.values)])\n",
    "del stan['Season']\n",
    "del stan['Episode']\n",
    "\n",
    "stan.Character[stan.Character != 'Stan'] = 'Not Stan'\n",
    "stan.Character[stan.Character == 'Stan'] = 'Stan'\n",
    "\n",
    "X_stan = stan.Line\n",
    "y_stan = stan.Character\n",
    "vect_stan =CountVectorizer(stop_words='english')\n",
    "Xdtm_stan = vect_stan.fit_transform(X_stan)\n",
    "nb_stan = MultinomialNB()\n",
    "nb_stan.fit(Xdtm_stan,y_stan)\n",
    "nb_stan.score(Xdtm_stan,y_stan)\n",
    "\n",
    "tokens_stan = vect_stan.get_feature_names()\n",
    "\n",
    "token_count_stan= nb_stan.feature_count_[0,:]\n",
    "token_count_not_stan = nb_stan.feature_count_[1, :]\n",
    "\n",
    "stan_tokens = pd.DataFrame({'token':tokens_stan, 'Stan':token_count_stan, 'Not_Stan':token_count_not_stan}).set_index('token')\n",
    "\n",
    "stan_tokens['Stan'] = stan_tokens.Stan + 1\n",
    "stan_tokens['Not_Stan'] = stan_tokens.Not_Stan + 1\n",
    "\n",
    "stan_tokens['Stan'] = stan_tokens.Stan / nb_stan.class_count_[0]\n",
    "stan_tokens['Not_Stan'] = stan_tokens.Not_Stan / nb_stan.class_count_[1]\n",
    "\n",
    "stan_tokens['spam_ratio'] = stan_tokens.Stan / stan_tokens.Not_Stan\n",
    "\n",
    "# examine the DataFrame sorted by spam_ratio\n",
    "stan_tokens.sort_values('spam_ratio', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Spamminess\" for Kyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kyle</th>\n",
       "      <th>Not_Kyle</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>jambu</th>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>42.174042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>willzy</th>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>31.387742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabba</th>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>25.974005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lowered</th>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>24.323588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dreidel</th>\n",
       "      <td>0.008163</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>21.743894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bother</th>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>21.362164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fruity</th>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>21.362164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mick</th>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>21.362164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ships</th>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>21.362164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clay</th>\n",
       "      <td>0.001929</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>19.501017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Kyle  Not_Kyle  spam_ratio\n",
       "token                                  \n",
       "jambu    0.001391  0.000033   42.174042\n",
       "willzy   0.002070  0.000066   31.387742\n",
       "gabba    0.000856  0.000033   25.974005\n",
       "lowered  0.000802  0.000033   24.323588\n",
       "dreidel  0.008163  0.000375   21.743894\n",
       "bother   0.000704  0.000033   21.362164\n",
       "fruity   0.000704  0.000033   21.362164\n",
       "mick     0.000704  0.000033   21.362164\n",
       "ships    0.000704  0.000033   21.362164\n",
       "clay     0.001929  0.000099   19.501017"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kyle = pd.DataFrame(South_Park_raw.loc[South_Park_raw['Character'].isin(top_speakers.index.values)])\n",
    "del kyle['Season']\n",
    "del kyle['Episode']\n",
    "\n",
    "kyle.Character[kyle.Character != 'Kyle'] = 'Not Kyle'\n",
    "kyle.Character[kyle.Character == 'Kyle'] = 'Kyle'\n",
    "\n",
    "X_kyle = kyle.Line\n",
    "y_kyle = kyle.Character\n",
    "\n",
    "vect_kyle = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 1), \n",
    "                       binary=False, lowercase=True, norm=None, smooth_idf=True, strip_accents=None,\n",
    "                       sublinear_tf=True, use_idf=False)\n",
    "\n",
    "#vect_kyle =CountVectorizer(stop_words='english')\n",
    "Xdtm_kyle = vect_kyle.fit_transform(X_kyle)\n",
    "nb_kyle = MultinomialNB()\n",
    "nb_kyle.fit(Xdtm_kyle,y_kyle)\n",
    "nb_kyle.score(Xdtm_kyle,y_kyle)\n",
    "\n",
    "tokens_kyle = vect_kyle.get_feature_names()\n",
    "\n",
    "token_count_kyle= nb_kyle.feature_count_[0,:]\n",
    "token_count_not_kyle = nb_kyle.feature_count_[1, :]\n",
    "\n",
    "kyle_tokens = pd.DataFrame({'token':tokens_kyle, 'Kyle':token_count_kyle, 'Not_Kyle':token_count_not_kyle}).set_index('token')\n",
    "\n",
    "kyle_tokens['Kyle'] = kyle_tokens.Kyle + 1\n",
    "kyle_tokens['Not_Kyle'] = kyle_tokens.Not_Kyle + 1\n",
    "\n",
    "kyle_tokens['Kyle'] = kyle_tokens.Kyle / nb_kyle.class_count_[0]\n",
    "kyle_tokens['Not_Kyle'] = kyle_tokens.Not_Kyle / nb_kyle.class_count_[1]\n",
    "\n",
    "kyle_tokens['spam_ratio'] = kyle_tokens.Kyle / kyle_tokens.Not_Kyle\n",
    "\n",
    "# examine the DataFrame sorted by spam_ratio\n",
    "kyle_tokens.sort_values('spam_ratio', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'sort'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-e7cdb247197b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mAll_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Token'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Token_Count'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtoken_count\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Token'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mAll_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Token_Count'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5065\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5066\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5067\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5069\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'sort'"
     ]
    }
   ],
   "source": [
    "tokens = vect.get_feature_names()\n",
    "\n",
    "token_count= nb_SP_Model.feature_count_[0,:]\n",
    "\n",
    "All_tokens = pd.DataFrame({'Token':tokens, 'Token_Count':token_count}).set_index('Token')\n",
    "All_tokens.sort(columns='Token_Count', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
